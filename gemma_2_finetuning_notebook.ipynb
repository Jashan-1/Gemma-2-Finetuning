{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Finetuning the latest GEMMA-2 models**"
      ],
      "metadata": {
        "id": "Eq2e1aXVCQF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the packages\n"
      ],
      "metadata": {
        "id": "P54xdhbCCxy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUgmWiGVwNI4"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes==0.43.3\n",
        "!pip install -q -U peft==0.12.0\n",
        "!pip install -q -U trl==0.10.1\n",
        "!pip install -q -U accelerate==0.34.2\n",
        "!pip install -q -U datasets==3.0.0\n",
        "!pip install -q -U transformers==4.44.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the necessary libraries\n"
      ],
      "metadata": {
        "id": "0Nh8ICO4C2Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import transformers\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, GemmaTokenizer"
      ],
      "metadata": {
        "id": "5AGSuJxCxbqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the Hugging Face APII token"
      ],
      "metadata": {
        "id": "6jxozV-UDJ_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "6O-BG2eSx-Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model selection and configuration"
      ],
      "metadata": {
        "id": "HR_QylCSDYm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/datagemma-rag-27b-it\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "UMZdtZyczm3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializations"
      ],
      "metadata": {
        "id": "m7sZCQe1Dl3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained (model_id, token=os.environ['HF_TOKEN'])\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                              quantization_config=bnb_config,\n",
        "                                              device_map={\"\":0},\n",
        "                                              token=os.environ ['HF_TOKEN'])"
      ],
      "metadata": {
        "id": "DeKTq4Agz0Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text generation and detokenization"
      ],
      "metadata": {
        "id": "a4J4eLB6DrYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Quote: Good things take time\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer (text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate (**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "GgE6IuDy0D_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enabling Weights & Biases"
      ],
      "metadata": {
        "id": "q6tjaXdqD17h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"false\""
      ],
      "metadata": {
        "id": "L5gy0Y201ulC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA configuration\n"
      ],
      "metadata": {
        "id": "78j_RkHHD6kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ã¾ora_config = LoraConfig(\n",
        "r == 8,\n",
        "target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
        "\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "task_type = \"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "gWEtDgvt1ug-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading dataset from Hugging Face"
      ],
      "metadata": {
        "id": "4cDg6T7GEC_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer (samples[\"quote\"]), batched=True)"
      ],
      "metadata": {
        "id": "Qqe-UWHk1ufK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "jYcqWz57EGlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['train']['quote']"
      ],
      "metadata": {
        "id": "KSjofPlZ1uby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "text = f\"Quote: {example['quote'][0]}\\nAuthor: {example['author'][0]}\"\n",
        "return [text]"
      ],
      "metadata": {
        "id": "i5XmGLho1uaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "  r = 8,\n",
        "  target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
        "  \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "  task_type = \"CAUSAL_LM\",\n",
        "  )"
      ],
      "metadata": {
        "id": "xs1GokT3-dfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Initialization"
      ],
      "metadata": {
        "id": "GilVReVqEQqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "model=model,\n",
        "train_dataset=data[\"train\"],\n",
        "args=transformers. TrainingArguments(\n",
        "per_device_train_batch_size=1,\n",
        "gradient_accumulation_steps=4,\n",
        "warmup_steps=2,\n",
        "max_steps=100,\n",
        "learning_rate=2e-4,\n",
        "fp16=True,\n",
        "logging_steps=1,\n",
        "output_dir=\"outputs\",\n",
        "optim=\"paged_adamw_8bit\"\n",
        "),\n",
        "peft_config=lora_config,\n",
        "formatting_func=formatting_func,\n",
        ")"
      ],
      "metadata": {
        "id": "PceO5aR61uP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "kZlJKapj23xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing by data generation"
      ],
      "metadata": {
        "id": "av3Yx9lYEZ0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Quote: Be yourself;\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer (text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print (tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "5GysnNi_23t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Quote: The opposite of love is not hate,\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "0Ze49OyI23r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Quote: So many books; so little time\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate (**inputs, max_new_tokens=20)\n",
        "print (tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ex2EJL3a23pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating BLEU and ROUGE scores"
      ],
      "metadata": {
        "id": "ZB6Ck6b9I4sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Load your fine-tuned model and tokenizer\n",
        "model_id = \"your_fine_tuned_model_id\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Prepare evaluation data\n",
        "eval_texts = [\"text 1\", \"text 2\", \"text 3\"]\n",
        "eval_references = [[\"reference 1.1\", \"reference 1.2\"], [\"reference 2.1\"], [\"reference 3.1\"]]\n",
        "\n",
        "# Generate text\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "generated_texts = generator(eval_texts, max_length=100, num_beams=4)\n",
        "\n",
        "# Calculate metrics\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "# BLEU\n",
        "bleu_scores = []\n",
        "for generated, references in zip(generated_texts, eval_references):\n",
        "    bleu_scores.append(sentence_bleu(references, generated[0], smoothing_function=SmoothingFunction().method1))\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "print(\"Average BLEU:\", average_bleu)\n",
        "\n",
        "# ROUGE\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(generated_texts, eval_references)\n",
        "print(\"ROUGE scores:\", scores)"
      ],
      "metadata": {
        "id": "bEoFcCXTInG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Successfully finetuned and implemented."
      ],
      "metadata": {
        "id": "YTlw-g8pEidG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nIsMX-6C23my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C4SGpUTO23jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nKarJAO23hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3WFVhQzU1ty4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}